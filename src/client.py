import time
from copy import deepcopy
from dataclasses import dataclass
from typing import Literal
from .setting import Settings
from langchain_together import ChatTogether
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage


@dataclass
class LLMUsage:
    duration: float
    """The time taken for the interaction in seconds."""

    num_token_in: int
    """The number of input tokens used in the LLM interaction."""

    num_token_out: int
    """The number of output tokens generated by the LLM."""

    @classmethod
    def from_metadata(cls, metadata: dict, duration: float) -> "LLMUsage":
        if metadata is None:
            metadata = {}
        return LLMUsage(
            duration=duration,
            num_token_in=metadata.get("input_tokens", 0),
            num_token_out=metadata.get("output_tokens", 0),
        )


@dataclass
class LLMResult:
    """Stores the results of an LLM interaction including content and performance metrics."""

    content: str
    """The textual output generated by the LLM."""

    usage: LLMUsage
    """Object containing LLM usage statistics."""

    @classmethod
    def from_message(cls, message: AIMessage | str, duration: float):
        """Creates an LLMResult from an AIMessage and duration.

        Args:
            message (AIMessage | str): The message object returned by the LLM.
            duration (float): The time taken for the interaction in seconds.

        Returns:
            LLMResult: A new LLMResult instance populated from the message and duration.
        """

        metadata = {} if isinstance(message, str) else message.usage_metadata
        usage = LLMUsage.from_metadata(metadata=metadata, duration=duration)
        content = message if isinstance(message, str) else message.content
        return LLMResult(content=content, usage=usage)


class LLMClient:
    """Client for interacting with multiple LLM models with prompt management."""

    def __init__(self, from_settings: Settings | None = None):
        api_config = deepcopy(from_settings if from_settings else Settings()).api_config
        self.__api_models = [
            ChatTogether(
                model=model.id,
                temperature=0,
                max_tokens=None,
                together_api_key=api_config.api_key,
            )
            for model in api_config.llm_models
        ]
        local_config = deepcopy(
            from_settings if from_settings else Settings()
        ).local_config
        self.__local_models = [
            OllamaLLM(model=model.id, temperature=0)
            for model in local_config.llm_models
        ]
        self.api_names = [model.name for model in api_config.llm_models]
        """Names of the available LLM API models."""

        self.local_names = [model.name for model in local_config.llm_models]
        """Names of the available LLM Local models."""

        self.n_api_models = len(self.__api_models)
        """Number of available LLM API models."""

        self.n_local_models = len(self.__local_models)
        """Number of available LLM Local models."""

        self.__prompt_key = None

    def inject_prompt(self, key: str, prompt: ChatPromptTemplate) -> None:
        """Injects a prompt template to be used with the LLM models.

        Args:
            key (str): Unique identifier for the prompt to avoid unnecessary re-initialization.
            prompt (ChatPromptTemplate): The prompt template to use with the LLM models.

        Note:
            If the same key is provided multiple times, the prompt will only be set once.
        """
        if self.__prompt_key == key:
            return
        self.__prompt_key = key
        self.__api_chains = [prompt | model for model in self.__api_models]
        self.__local_chains = [prompt | model for model in self.__local_models]

    def run(
        self,
        values: dict,
        source: Literal["api", "local"] = "api",
        model_idxs: list[int] = [],
    ) -> dict[int, LLMResult]:
        """Executes the LLM interaction with the provided input values.

        Args:
            values (dict): Input values to be used with the injected prompt template.
            source ('api' | 'local'): Source model used.
            model_idxs (list[int]): Optional list of model indices to use. If empty,
                all available models will be used.

        Returns:
            dict[int,LLMResult]: Dictionary mapping model indices to their results.

        Raises:
            ValueError: If any model index in model_idxs is out of bounds.
            NotImplementedError: If no prompt has been injected before running.
        """
        n = len(model_idxs)
        m = self.n_api_models if source == "api" else self.n_local_models
        used_model_idxs = model_idxs[:] if n > 0 else list(range(m))
        for i in used_model_idxs:
            if i < 0 or i >= m:
                raise ValueError(
                    f"Invalid model index: {i}. Must be between 0 and {len(m) - 1}."
                )
        attr = (
            "_LLMClient__api_chains" if source == "api" else "_LLMClient__local_chains"
        )
        if not hasattr(self, attr):
            raise NotImplementedError(
                "There is no prompt injected yet. Please inject the prompt first"
            )
        results: dict[int, LLMResult] = {}
        for i in used_model_idxs:
            time.sleep(1)
            start_time = time.time()
            message = (
                self.__api_chains[i].invoke(values)
                if source == "api"
                else self.__local_chains[i].invoke(values)
            )
            end_time = time.time()
            duration = end_time - start_time
            results[i] = LLMResult.from_message(message, duration)
        return results
